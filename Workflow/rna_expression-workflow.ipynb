{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monthly-offer",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# RNA-seq expression - Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-frame",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "passing-measurement",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files.\n",
    "parameter: cwd = path(\"output\")\n",
    "# Sample meta data list\n",
    "parameter: samples = path\n",
    "# Raw data directory, default to the same directory as sample list\n",
    "parameter: data_dir = path(f\"{samples:d}\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Memory for Java virtual mechine (`picard`)\n",
    "parameter: java_mem = \"6G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')\n",
    "## Whether the fasta/fastq file is compressed or not.\n",
    "parameter: uncompressed = False\n",
    "import os\n",
    "import pandas as pd\n",
    "## FIX: The way to get sample needs to be revamped to 1. Accomodate rf/fr as column 2. accomodate single end read (Only 2 fq/samples)\n",
    "sample_inv = pd.read_csv(samples,\"\\t\")\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_inv = []\n",
    "\n",
    "if \"strand\" in sample_inv.columns:\n",
    "    strand_inv = sample_inv.strand.values.tolist()\n",
    "    sample_inv = sample_inv.drop(\"strand\" , axis = 1)\n",
    "    stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\",\"strand_missing\"] for x in strand_inv ]), msg = \"strand columns should only include ``fr``, ``rf``, ``strand_missing`` or ``unstranded``\")\n",
    "            \n",
    "## Extract read_length if user have specified read_length\n",
    "read_length = []\n",
    "if \"read_length\" in sample_inv.columns:\n",
    "    read_length = sample_inv.read_length.values.tolist()\n",
    "    sample_inv = sample_inv.drop(\"read_length\" , axis = 1)\n",
    "    \n",
    "    \n",
    "## Extract sample_id\n",
    "sample_inv_list = sample_inv.values.tolist()\n",
    "sample_id = [x[0] for x in sample_inv_list]\n",
    "\n",
    "\n",
    "    \n",
    "## Get the file name for single/paired end data\n",
    "file_inv = [x[1:] for x in sample_inv_list]\n",
    "file_inv = [item for sublist in file_inv for item in sublist]\n",
    "\n",
    "fastq = [f'{data_dir}/{x}' for x in file_inv]\n",
    "\n",
    "\n",
    "for y in fastq:\n",
    "        if not os.path.isfile(y):\n",
    "            raise ValueError(f\"File {y} does not exist\")\n",
    "\n",
    "if len(fastq) != len(set(fastq)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in fastq file list\")\n",
    "\n",
    "# Is the RNA-seq data pair-end\n",
    "is_paired_end = 0 if len(fastq) == len(sample_id) else 1 \n",
    "from sos.utils import env\n",
    "env.logger.info(f'Input samples are {\"paired-end\" if is_paired_end else \"single-end\"} sequences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-commission",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 0: QC before alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "refined-bidder",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastqc]\n",
    "input: fastq, group_by =  1\n",
    "output: f'{cwd}/{_input:bn}_fastqc.html',f'{cwd}/{_input:bn}_fastqc/fastqc_data.txt' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    fastqc ${_input} -o ${_output[0]:d}\n",
    "    unzip -o ${_output[0]:n}.zip -d ${cwd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-chosen",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 1: Remove adaptor through `fastp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-handbook",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastp_trim_adaptor_1]\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# the mean quality requirement option for cut_front\n",
    "parameter: leading = 20\n",
    "# the mean quality requirement option for cut_tail\n",
    "parameter: trailing = 20\n",
    "# reads shorter than length_required will be discarded\n",
    "parameter: min_len = 15\n",
    "# Path to the reference adaptors\n",
    "parameter: fasta_with_adapters_etc = path(\".\")\n",
    "warn_if(fasta_with_adapters_etc.is_file(),msg = \"Use input fasta and adaptor detection of paired-end read was disabled\" )\n",
    "\n",
    "input: fastq, group_by = is_paired_end + 1 , group_with = \"sample_id\"\n",
    "output: [f'{cwd}/{path(x):bnn}.trimmed.fq.gz' for x in _input]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash:  container=container,expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "        fastp -i ${f'{_input[0]} -I {_input[1]}' if is_paired_end else _input} -o ${ f'{_output[0]} -O {_output[1]}' if is_paired_end else _output } \\\n",
    "            ${f'--adapter_fasta {fasta_with_adapters_etc}' if fasta_with_adapters_etc.is_file() else \"--detect_adapter_for_pe\"}  -V -h ${_output[0]:n}.html -j ${_output[0]:n}.json -w ${numThreads} \\\n",
    "            --length_required ${min_len}  -W ${window_size} -M ${required_quality} -5 -3 --cut_front_mean_quality ${leading} --cut_tail_mean_quality ${leading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e84b1-027a-473d-807d-eb189f309690",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastp_trim_adaptor_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:n}.trimmed.txt'\n",
    "sample_inv_tmp = pd.read_csv(samples,\"\\t\")\n",
    "if is_paired_end:\n",
    "    sample_inv_tmp.fq1 = [f'{x:r}' for x in _input][::2]\n",
    "    sample_inv_tmp.fq2 = [f'{x:r}' for x in _input][1::2]\n",
    "else:\n",
    "    sample_inv_tmp.fq1 = [f'{x:r}' for x in _input]\n",
    "sample_inv_tmp.to_csv(_output,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-shooting",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 1 Alternative: Remove adaptor through `Trimmomatic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-alfred",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[trimmomatic_trim_adaptor]\n",
    "# Path to the software. Default set to using our rna_quantification.sif image\n",
    "parameter: trimmomatic_jar = \"/opt/Trimmomatic-0.39/trimmomatic-0.39.jar\"\n",
    "# Illumina clip setting\n",
    "# Path to the reference adaptors\n",
    "parameter: fasta_with_adapters_etc = path(\".\")\n",
    "parameter: seed_mismatches = 2\n",
    "parameter: palindrome_clip_threshold = 30\n",
    "parameter: simple_clip_threshold = 10\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# Other settings\n",
    "parameter: leading = 3\n",
    "parameter: trailing = 3\n",
    "parameter: min_len = 50\n",
    "input: fastq, group_by = 2, group_with = \"sample_id\"\n",
    "output: fq_1 = f'{cwd}/{_sample_id}_paired_{_input[0]:bn}.gz',\n",
    "        fq_1_up = f'{cwd}/{_sample_id}_unpaired_{_input[0]:bn}.gz',\n",
    "        fq_2 = f'{cwd}/{_sample_id}_paired_{_input[1]:bn}.gz',\n",
    "        fq_2_up = f'{cwd}/{_sample_id}_unpaired_{_input[1]:bn}.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    java -jar -Xmx${java_mem} ${trimmomatic_jar} PE -threads ${numThreads} \\\n",
    "        ${_input[0]} \\\n",
    "        ${_input[1]} \\\n",
    "        ${_output[0]} \\\n",
    "        ${_output[1]} \\\n",
    "        ${_output[2]} \\\n",
    "        ${_output[3]} \\\n",
    "        ILLUMINACLIP:${fasta_with_adapters_etc}:${seed_mismatches}:${palindrome_clip_threshold}:${simple_clip_threshold} \\\n",
    "        LEADING:${leading} TRAILING:${trailing} SLIDINGWINDOW:${window_size}:${required_quality} MINLEN:${min_len}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-necessity",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 2: Alignment through `STAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "documented-malawi",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_align_1]\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "# STAR indexing file\n",
    "parameter: STAR_index = path\n",
    "parameter: outFilterMultimapNmax = 20 \n",
    "parameter: alignSJoverhangMin = 8 \n",
    "parameter: alignSJDBoverhangMin = 1 \n",
    "parameter: outFilterMismatchNmax = 999 \n",
    "parameter: outFilterMismatchNoverLmax = 0.1\n",
    "parameter: alignIntronMin = 20 \n",
    "parameter: alignIntronMax = 1000000 \n",
    "parameter: alignMatesGapMax = 1000000 \n",
    "parameter: outFilterType =  \"BySJout\" \n",
    "parameter: outFilterScoreMinOverLread = 0.33 \n",
    "parameter: outFilterMatchNminOverLread = 0.33 \n",
    "parameter: limitSjdbInsertNsj = 1200000 \n",
    "parameter: outSAMstrandField = \"intronMotif\" \n",
    "parameter: outFilterIntronMotifs = \"None\" \n",
    "parameter: alignSoftClipAtReferenceEnds = \"Yes\" \n",
    "parameter: quantMode = [\"TranscriptomeSAM\", \"GeneCounts\"]\n",
    "parameter: outSAMattrRGline = [\"ID:rg1\", \"SM:sm1\"]\n",
    "parameter: outSAMattributes = [\"NH\", \"HI\", \"AS\", \"nM\", \"NM\", \"ch\"] \n",
    "parameter: chimSegmentMin = 15 \n",
    "parameter: chimJunctionOverhangMin = 15 \n",
    "parameter: chimOutType = [\"Junctions\", \"WithinBAM\", \"SoftClip\"]\n",
    "parameter: chimMainSegmentMultNmax = 1 \n",
    "parameter: sjdbOverhang = 0\n",
    "if int(mem.replace(\"G\",\"\")) <  40:\n",
    "    print(\"Insufficent memory for STAR, changing to 40G\")\n",
    "    star_mem = '40G'\n",
    "else:\n",
    "    star_mem = mem\n",
    "# This option is commented out because it will force the downstream analysis to use 40G, which significantlly slow down the process.\n",
    "input: fastq,group_by = is_paired_end + 1,group_with = {\"sample_id\",\"read_length\"}\n",
    "output: cord_bam = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.out.bam',\n",
    "        trans_bam = f'{cwd}/{_sample_id}.Aligned.toTranscriptome.out.bam'\n",
    "if sjdbOverhang == 0:\n",
    "    print(\"Using read length specified in the sample list\")\n",
    "else:\n",
    "    print(\"Using specified --sjdbOverhang as read length\")\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = star_mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    rm -rf ${cwd}/${_sample_id}.*.out.*.gz\n",
    "    rm -rf ${cwd}/${_sample_id}._STARpass1\n",
    "    set -e\n",
    "    touch ${_output[0]:n}.star_start.timestamp\n",
    "    STAR --runMode alignReads \\\n",
    "        --runThreadN ${numThreads} \\\n",
    "        --genomeDir  ${STAR_index} \\\n",
    "        --readFilesIn  ${_input:r} \\\n",
    "        --readFilesCommand ${\"cat\" if uncompressed else \"zcat\"} \\\n",
    "        --outFileNamePrefix ${_output[0]:nnnn}. \\\n",
    "        --outSAMstrandField ${outSAMstrandField} \\\n",
    "        --twopassMode Basic \\\n",
    "        --outFilterMultimapNmax ${outFilterMultimapNmax} \\\n",
    "        --alignSJoverhangMin ${alignSJoverhangMin} \\\n",
    "        --alignSJDBoverhangMin ${alignSJDBoverhangMin} \\\n",
    "        --outFilterMismatchNmax ${outFilterMismatchNmax} \\\n",
    "        --outFilterMismatchNoverLmax ${outFilterMismatchNoverLmax} \\\n",
    "        --alignIntronMin ${alignIntronMin} \\\n",
    "        --alignIntronMax ${alignIntronMax} \\\n",
    "        --alignMatesGapMax ${alignMatesGapMax} \\\n",
    "        --outFilterType ${outFilterType} \\\n",
    "        --outFilterScoreMinOverLread ${outFilterScoreMinOverLread} \\\n",
    "        --outFilterMatchNminOverLread ${outFilterMatchNminOverLread} \\\n",
    "        --limitSjdbInsertNsj ${limitSjdbInsertNsj} \\\n",
    "        --outFilterIntronMotifs ${outFilterIntronMotifs} \\\n",
    "        --alignSoftClipAtReferenceEnds ${alignSoftClipAtReferenceEnds} \\\n",
    "        --quantMode ${\" \".join(quantMode)} \\\n",
    "        --outSAMtype BAM Unsorted \\\n",
    "        --outSAMunmapped Within \\\n",
    "        --genomeLoad NoSharedMemory \\\n",
    "        --chimSegmentMin ${chimSegmentMin} \\\n",
    "        --chimJunctionOverhangMin ${chimJunctionOverhangMin} \\\n",
    "        --chimOutType ${\" \".join(chimOutType)} \\\n",
    "        --chimMainSegmentMultNmax ${chimMainSegmentMultNmax} \\\n",
    "        --chimOutJunctionFormat 0 \\\n",
    "        --outSAMattributes ${\" \".join(outSAMattributes)} \\\n",
    "        --outSAMattrRGline ${\" \".join(outSAMattrRGline)} \\\n",
    "        --sjdbOverhang ${sjdbOverhang if sjdbOverhang != 0 else _read_length} \\\n",
    "        --sjdbGTFfile ${gtf} \\\n",
    "\n",
    "    rm -r ${_output[0]:nnnn}._STARgenome\n",
    "    rm -r ${_output[0]:nnnn}._STARtmp\n",
    "    touch ${_output[0]:n}.sort_start.timestamp\n",
    "    samtools sort --threads ${numThreads} -o ${_output[0]:nnnn}.Aligned.sortedByCoord.out.bam ${_output[0]:nnnn}.Aligned.out.bam # According to GTEX, this can help reducing the amount of memory consumption.\n",
    "    rm ${_output[0]:nnnn}.Aligned.out.bam \n",
    "    samtools index ${_output[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2a8f8-8c3f-4b60-aa31-b6e438530d38",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[strand_detected_1: shared=\"step_strand_detected\"]\n",
    "input: output_from(\"STAR_align_1\")[\"trans_bam\"], group_with = \"sample_id\"\n",
    "import pandas as pd\n",
    "ReadsPerGene = pd.read_csv(f'{cwd}/{_sample_id}.ReadsPerGene.out.tab' , sep = \"\\t\" , header = None )\n",
    "ReadsPerGene_list = ReadsPerGene.loc[3::,].sum(axis = 0, numeric_only = True).values.tolist()\n",
    "strand_percentage = [x/ReadsPerGene_list[0] for x in ReadsPerGene_list]\n",
    "print(f'for sample {_sample_id}')\n",
    "if strand_percentage[1] > 0.9:\n",
    "    print(f'Counts for the 1st read strand aligned with RNA is {strand_percentage[1]}, > 90% of aligned count')\n",
    "    print('Data is likely FR/fr-secondstrand')\n",
    "    strand_detected = \"fr\"\n",
    "elif  strand_percentage[2] > 0.9:\n",
    "    print(f'Counts for the 2nd read strand aligned with RNA is {strand_percentage[2]}, > 90% of aligned count')\n",
    "    print('Data is likely RF/fr-firststrand')\n",
    "    strand_detected = \"rf\"\n",
    "elif max( strand_percentage[1],  strand_percentage[2]) < 0.6:\n",
    "    print(f'Both {strand_percentage[1]} and  {strand_percentage[2]} are under 60% of reads explained by one direction')\n",
    "    print('Data is likely unstranded')\n",
    "    strand_detected = \"unstranded\"\n",
    "else:\n",
    "    print(f'Data does not fall into a likely stranded (max percent explained {max( strand_percentage[1],  strand_percentage[2])} > 0.9) or unstranded layout (max percent explained {max( strand_percentage[1],  strand_percentage[2])} < 0.6)')\n",
    "    sys.exit('Exiting, please check your data and manually specified the ``--strand`` option as ``fr``, ``rf`` or ``unstranded``')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a896c7-a922-4f90-9c37-bffc1fdeb1bc",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[strand_detected_2: shared=\"strand\"]\n",
    "input: group_by = \"all\"\n",
    "parameter: strand = \"\"\n",
    "import pandas as pd\n",
    "if not strand:\n",
    "    if len(strand_inv) > 0:\n",
    "        strand = strand_inv\n",
    "        for i in range(0,len(strand)):\n",
    "            if strand[i] == \"strand_missing\":\n",
    "                strand[i] = step_strand_detected[i]\n",
    "        print(f'Using strand specified in the input samples list {strand}, replacing strand_missing with detected strand')\n",
    "    else:\n",
    "        warn_if(not all(x is step_strand_detected[0] for x in step_strand_detected), msg = \"strands detected are different among samples, please check your protocol, we will use the detected strand for each samples\")    \n",
    "        strand = step_strand_detected\n",
    "        print(f'Using detected strand for each samples {strand}')\n",
    "else:\n",
    "    stop_if(strand not in [\"fr\", \"rf\", \"unstranded\"], msg = \"``--strand`` option should be ``fr``, ``rf`` or ``unstranded``\")\n",
    "    print(f'Using ``--strand`` overwrite option for all the samples {strand[0]}') \n",
    "    strand = [strand] * len(step_strand_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-baking",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 3: Mark duplicates reads & QC through `Picard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19088a92-c333-400c-9f5d-39715a33e75f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[picard_qc]\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "depends: sos_variable('strand')\n",
    "# Path to flat reference file, for computing QC metric\n",
    "parameter: ref_flat = path\n",
    "# Path to the software. Default set to using our rna_quantification.sif image\n",
    "parameter: picard_jar = \"/opt/picard-tools/picard.jar\"\n",
    "# The fasta reference file used to generate star index\n",
    "parameter: reference_fasta = path\n",
    "# For the patterned flowcell models (HiSeq X), change to 2500\n",
    "parameter: optical_distance = 100\n",
    "picard_strand_dict = {\"rf\":\"SECOND_READ_TRANSCRIPTION_STRAND\",\"fr\": \"FIRST_READ_TRANSCRIPTION_STRAND\",\"unstranded\":\"NONE\" }\n",
    "input: output_from(\"STAR_align_1\"),group_by = 2, group_with = {\"sample_id\",\"strand\"}\n",
    "output: picard_metrics = f'{cwd}/{_sample_id}.alignment_summary_metrics',\n",
    "        picard_rna_metrics = f'{cwd}/{_sample_id}.rna_metrics',\n",
    "        md_bam = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.out.md.bam',\n",
    "        md_metrics = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.md.metrics'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "        set -e\n",
    "        touch ${_output[0]:n}.CollectMultipleMetrics_start.timestamp\n",
    "        java -jar -Xmx${java_mem} ${picard_jar} CollectMultipleMetrics \\\n",
    "            -REFERENCE_SEQUENCE ${reference_fasta} \\\n",
    "            -PROGRAM CollectAlignmentSummaryMetrics \\\n",
    "            -PROGRAM CollectInsertSizeMetrics \\\n",
    "            -PROGRAM QualityScoreDistribution \\\n",
    "            -PROGRAM MeanQualityByCycle \\\n",
    "            -PROGRAM CollectBaseDistributionByCycle \\\n",
    "            -PROGRAM CollectGcBiasMetrics \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -INPUT  ${_input[\"cord_bam\"]} \\\n",
    "            -OUTPUT  ${_output[0]:n}\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout'\n",
    "        set -e\n",
    "        touch ${_output[2]:n}.MarkDuplicates_start.timestamp\n",
    "        java -Xmx${java_mem} -jar ${picard_jar} MarkDuplicates \\\n",
    "            -I ${_input[\"cord_bam\"]}  \\\n",
    "            -O ${_output[2]} \\\n",
    "            -PROGRAM_RECORD_ID null \\\n",
    "            -M ${_output[3]} \\\n",
    "            -TMP_DIR ${cwd}\\\n",
    "            -MAX_RECORDS_IN_RAM 500000 -SORTING_COLLECTION_SIZE_RATIO 0.25 \\\n",
    "            -ASSUME_SORT_ORDER coordinate \\\n",
    "            -TAGGING_POLICY DontTag \\\n",
    "            -OPTICAL_DUPLICATE_PIXEL_DISTANCE ${optical_distance} \\\n",
    "            -CREATE_INDEX true \\\n",
    "            -CREATE_MD5_FILE true \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -REMOVE_SEQUENCING_DUPLICATES false \\\n",
    "            -REMOVE_DUPLICATES false \n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stderr'\n",
    "        set -e\n",
    "        # Get only line with rRNA and transcript_id\n",
    "        cat ${gtf}| grep rRNA | grep transcript_id > ${gtf}.tmp.${_input[\"cord_bam\"]:bnnnn}  # To Avoid data racing problem\n",
    "        samtools view -H ${_input[\"cord_bam\"]}  > ${_input[\"cord_bam\"]}.RI  \n",
    "\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stderr'\n",
    "        import pandas as pd\n",
    "        from collections import defaultdict\n",
    "        chrom = []\n",
    "        start = []\n",
    "        end = []\n",
    "        strand = []\n",
    "        tag = []\n",
    "        annotation_gtf = \"${gtf}.tmp.${_input[\"cord_bam\"]:bnnnn}\"\n",
    "        with open(annotation_gtf, 'r') as gtf:\n",
    "            for row in gtf:\n",
    "                row = row.strip().split('\\t')\n",
    "                if row[0][0]=='#' or row[2]!=\"transcript\": continue # skip header\n",
    "                chrom.append(row[0])\n",
    "                start.append(row[3])\n",
    "                end.append(row[4])\n",
    "                strand.append(row[6])\n",
    "                attributes = defaultdict()\n",
    "                for a in row[8].replace('\"', '').split(';')[:-1]:\n",
    "                    kv = a.strip().split(' ')\n",
    "                    if kv[0]!='tag':\n",
    "                        attributes[kv[0]] = kv[1]\n",
    "                    else:\n",
    "                        attributes.setdefault('tags', []).append(kv[1])\n",
    "                tag.append(attributes)\n",
    "        transcript_id = [x[\"transcript_id\"] for x in tag]\n",
    "        RI = pd.DataFrame(data={'chr':chrom, 'start':start, 'end':end, 'strand':strand,'transcript_id' : transcript_id })\n",
    "        RI.to_csv(\"${_input[\"cord_bam\"]}.RI\", index = 0, header = 0, mode = \"a\",sep = \"\\t\" )\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stdout'\n",
    "        set -e\n",
    "        touch ${_output[0]:n}.CollectRnaSeqMetrics_start.timestamp\n",
    "        java -jar -Xmx${java_mem} ${picard_jar} CollectRnaSeqMetrics \\\n",
    "            -REF_FLAT ${ref_flat} \\\n",
    "            -RIBOSOMAL_INTERVALS ${_input[\"cord_bam\"]}.RI \\\n",
    "            -STRAND_SPECIFICITY ${picard_strand_dict[_strand]} \\\n",
    "            -CHART_OUTPUT ${_output[0]:n}.rna_metrics.pdf \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -INPUT ${_input[\"cord_bam\"]}  \\\n",
    "            -OUTPUT  ${_output[0]:n}.rna_metrics\n",
    "        rm ${gtf}.tmp.${_input[\"cord_bam\"]:bnnnn}\n",
    "\n",
    "_input[\"cord_bam\"].zap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edeb27-56e6-4e80-b116-cc913d6fe0e3",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_output]\n",
    "input: output_from(\"picard_qc\"),  group_by = \"all\"\n",
    "depends: sos_variable(\"strand\")\n",
    "output: f'{cwd}/{samples:bn}_bam_list'\n",
    "import pandas as pd\n",
    "coord_bam_list = [f'{x:b}' for x in _input[2::4]]\n",
    "SJ_list = [f'{x:bnnnnn}.SJ.out.tab' for x in _input[2::4]]\n",
    "Trans_bam_list = [f'{x:bnnnn}.toTranscriptome.out.bam' for x in _input[2::4]]\n",
    "print(SJ_list)\n",
    "out = pd.DataFrame({\"sample_id\" : sample_id,\"strand\" : strand , \"coord_bam_list\" : coord_bam_list, \"SJ_list\" :SJ_list,\"trans_bam_list\" : Trans_bam_list  })\n",
    "out.to_csv(_output,sep = \"\\t\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266d430-2c34-4b13-a816-7729703d3e55",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 4: Post aligment QC through `RNA-SeQC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1538e84-7294-4ca1-a6c2-3f7f60f47e1a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_1]\n",
    "import os\n",
    "import pandas as pd\n",
    "parameter: bam_list = path\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "sample_inv = pd.read_csv(bam_list,\"\\t\")\n",
    "\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_list = sample_inv.strand.values.tolist()\n",
    "stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\"] for x in strand_list ]), msg = \"strand columns should only include ``fr``, ``rf`` or ``unstranded``, please check the bam_list\")     \n",
    "## Extract sample_id\n",
    "sample_id = sample_inv.sample_id.values.tolist()\n",
    "    \n",
    "## Get the file name for cood_bam data\n",
    "coord_bam_list_inv = sample_inv.coord_bam_list.values.tolist()\n",
    "coord_bam_list = [f'{cwd}/{x}' for x in coord_bam_list_inv ]\n",
    "input:  coord_bam_list, group_by = 1, group_with = {\"sample_id\",\"strand_list\"}\n",
    "output: f'{cwd}/{_sample_id}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.gene_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.exon_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.metrics.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    cd ${cwd} && \\\n",
    "    run_rnaseqc.py \\\n",
    "        ${gtf:a} \\\n",
    "        ${_input:a} \\\n",
    "        ${_sample_id}.rnaseqc \\\n",
    "        -o ./ \\\n",
    "        ${(\"--stranded \" + _strand_list) if _strand_list != \"unstranded\" else \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596adc2a-2f86-4ccf-a298-9d2199e8f4ca",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:bn}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.gene_readsCount.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.exon_readsCount.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.metrics.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    def make_gct(gct_path):\n",
    "        # sample_name\n",
    "        sample_name = \".\".join(os.path.basename(gct_path).split(\".\")[:-4])\n",
    "        # read_input\n",
    "        pre_gct = pd.read_csv(gct_path,sep = \"\\t\",\n",
    "                              skiprows= 2,index_col=\"Name\").drop(\"Description\",axis = 1)\n",
    "        pre_gct.index.name = \"gene_ID\"\n",
    "        pre_gct.columns = [sample_name]\n",
    "        return(pre_gct)\n",
    "\n",
    "    def merge_gct(gct_path_list):\n",
    "        gct = pd.DataFrame()\n",
    "        for gct_path in gct_path_list:\n",
    "            #check duplicated indels and remove them.\n",
    "            gct_col = make_gct(gct_path)\n",
    "            gct = gct.merge(gct_col,right_index=True,left_index = True,how = \"outer\")\n",
    "        return gct\n",
    "\n",
    "    input_list = [${_input:r,}]\n",
    "    tpm_list = input_list[0::4]\n",
    "    gc_list = input_list[1::4]\n",
    "    ec_list = input_list[2::4]\n",
    "    gct_path_list_list = [tpm_list,gc_list,ec_list]\n",
    "    output_path = [${_output:r,}][0:3]\n",
    "    for i in range(0,len(output_path)):\n",
    "        output = merge_gct(gct_path_list_list[i])\n",
    "        output.to_csv(output_path[i], sep = \"\\t\")\n",
    "    metrics_list = input_list[3::4]\n",
    "    with open(\"${cwd}/${samples:bn}.rnaseqc.metrics_output_list\", \"w\") as f:\n",
    "        f.write('\\n'.join(metrics_list))\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    aggregate_rnaseqc_metrics.py  ${_output[3]:n}_output_list ${_output[3]:nn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7df23-0c6c-4fe3-9aa0-4a597bd99ef8",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 6: Quantify expression through `RSEM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b8bbdc-49f3-432d-98bb-809b05ee8dcf",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_1]\n",
    "parameter: RSEM_index = path\n",
    "parameter: max_frag_len = 1000\n",
    "estimate_rspd = True\n",
    "\n",
    "parameter: bam_list = path\n",
    "\n",
    "sample_inv = pd.read_csv(bam_list,\"\\t\")\n",
    "\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_list = sample_inv.strand.values.tolist()\n",
    "stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\"] for x in strand_list ]), msg = \"strand columns should only include ``fr``, ``rf`` or ``unstranded``, please check the bam_list\")     \n",
    "## Extract sample_id\n",
    "sample_id = sample_inv.sample_id.values.tolist()\n",
    "    \n",
    "## Get the file name for trans_bam_list data\n",
    "trans_bam_list_inv = sample_inv.trans_bam_list.values.tolist()\n",
    "trans_bam_list = [f'{cwd}/{x}' for x in trans_bam_list_inv ]\n",
    "input: trans_bam_list ,  group_by = 1, group_with = {\"sample_id\",\"strand_list\"} \n",
    "output: f'{cwd}/{_sample_id}.rsem.isoforms.results', f'{cwd}/{_sample_id}.rsem.genes.results',f'{cwd}/{_sample_id}.rsem.stat/{_sample_id}.rsem.cnt'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    run_RSEM.py ${RSEM_index:a} ${_input:a} ${_sample_id} \\\n",
    "        -o ${_output[0]:d} \\\n",
    "        --max_frag_len ${max_frag_len} \\\n",
    "        --estimate_rspd ${'true' if estimate_rspd else 'false'} \\\n",
    "        --paired_end ${\"true\" if is_paired_end else \"false\"} \\\n",
    "        --is_stranded ${\"true\" if _strand_list != \"unstranded\" else \"false\"} \\\n",
    "        --threads ${numThreads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5d6cc-8feb-452c-95cb-91fcf9a619e7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:bn}.rsem_transcripts_expected_count.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_tpm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_fpkm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_isopct.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_expected_count.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_tpm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_fpkm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem.aggregated_quality.metrics.tsv'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    input_list = [${_input:r,}]\n",
    "    with open('${cwd}/${samples:bn}.rsem.isoforms_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[0::3]))\n",
    "    with open('${cwd}/${samples:bn}.rsem.genes_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[1::3]))\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "         aggregate_rsem_results.py ${cwd}/${samples:bn}.rsem.isoforms_output_list {expected_count,TPM,FPKM,IsoPct} ${_output[0]:nnn}\n",
    "         aggregate_rsem_results.py ${cwd}/${samples:bn}.rsem.genes_output_list {expected_count,TPM,FPKM} ${_output[1]:nnn} \n",
    "\n",
    "R:  container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout'\n",
    "     readRSEM.cnt <- function (source) {\n",
    "            # RSEM .cnt files gives statistics about the (transcriptome) alignment passed to RSEM:\n",
    "            # Row 1: N0 (# unalignable reads);\n",
    "            #        N1 (# alignable reads);\n",
    "            #        N2 (# filtered reads due to too many alignments);\n",
    "            #        N_tot (N0+N1+N2)\n",
    "            # Row 2: nUnique (# reads aligned uniquely to a gene);\n",
    "            #        nMulti (# reads aligned to multiple genes);\n",
    "            #        nUncertain (# reads aligned to multiple locations in the given reference sequences, which include isoform-level multi-mapping reads)\n",
    "            # Row 3: nHits (# total alignments);\n",
    "            #        read_type (0: single-end read, no quality; 1: single-end read, with quality score; 2: paired-end read, no quality score; 3: paired-end read, with quality score)\n",
    "            # Source: https://groups.google.com/forum/#!topic/rsem-users/usmPKgsC5LU\n",
    "            # Note: N1 = nUnique + nMulti\n",
    "\n",
    "            stopifnot(file.exists(source[1]))\n",
    "            isDir <- file.info(source)$isdir\n",
    "            if (isDir) {\n",
    "                files <- system(paste(\"find\", source, \"-name \\\"*.rsem.cnt\\\"\"), intern=TRUE)\n",
    "                stopifnot(length(files) > 0)\n",
    "                samples <- gsub(\"_rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            } else {\n",
    "                files <- source\n",
    "                samples <- gsub(\".rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            }\n",
    "            metrics <- list()\n",
    "            for (i in 1:length(files)) {\n",
    "                m <- read.table(files[i], header=FALSE, sep=\" \", comment.char=\"#\", stringsAsFactors=FALSE, nrows=3, fill=TRUE)\n",
    "                metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                File=files[i],\n",
    "                TotalReads=m[1, 4],\n",
    "                AlignedReads=m[1, 2],\n",
    "                UniquelyAlignedReads=m[2, 1],\n",
    "                stringsAsFactors=FALSE)\n",
    "            }\n",
    "            metrics <- do.call(rbind, metrics)\n",
    "            row.names(metrics) <- metrics$Sample\n",
    "\n",
    "            return(metrics)\n",
    "        }\n",
    "        sourceRSEM = c(${_input:r,})\n",
    "        sourceRSEM = sourceRSEM[seq(3,length(sourceRSEM),3)]\n",
    "        metrics.RSEM = readRSEM.cnt(sourceRSEM)\n",
    "        write.table(metrics.RSEM, file=\"${_output[-1]}\",col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fab047-76ce-4fb9-9dab-ab121dbf9550",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 7: summarize with MultiQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d7905-b84b-40f3-8fad-e61e339452ee",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_3,rnaseqc_call_3]\n",
    "output: f'{cwd}/{samples:bn}.multiqc_report.html'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "report: output = f\"{_output:n}.multiqc_config.yml\"\n",
    "  extra_fn_clean_exts:\n",
    "      - '_rsem'\n",
    "  fn_ignore_dirs:\n",
    "      - '*_STARpass1'\n",
    "bash:  container=container,expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    multiqc ${_input:d} -v -n ${_output:b} -o ${_output:d} -c ${_output:n}.multiqc_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac480f9-ffc1-4a63-9a14-4ea6147121b7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_4, rnaseqc_call_4]\n",
    "# Path to flat reference file, for computing QC metric\n",
    "output: f'{cwd}/{samples:b}.picard.aggregated_quality.metrics.tsv'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    ## Define Function\n",
    "    readPicard.alignment_summary_metrics <- function (source) {\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.alignment_summary_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=2)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PF_READS=sum(m$PF_READS[1:2]),\n",
    "                                   PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2]),\n",
    "                                   PCT_PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2])/sum(m$PF_READS[1:2]),\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    readPicard.rna_metrics <- function(source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.rna_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PCT_RIBOSOMAL_BASES=m$PCT_RIBOSOMAL_BASES,\n",
    "                                   PCT_CODING_BASES=m$PCT_CODING_BASES,\n",
    "                                   PCT_UTR_BASES=m$PCT_UTR_BASES,\n",
    "                                   PCT_INTRONIC_BASES=m$PCT_INTRONIC_BASES,\n",
    "                                   PCT_INTERGENIC_BASES=m$PCT_INTERGENIC_BASES,\n",
    "                                   PCT_MRNA_BASES=m$PCT_MRNA_BASES,\n",
    "                                   PCT_USABLE_BASES=m$PCT_USABLE_BASES,\n",
    "                                   MEDIAN_CV_COVERAGE=m$MEDIAN_CV_COVERAGE,\n",
    "                                   MEDIAN_5PRIME_BIAS=m$MEDIAN_5PRIME_BIAS,\n",
    "                                   MEDIAN_3PRIME_BIAS=m$MEDIAN_3PRIME_BIAS,\n",
    "                                   MEDIAN_5PRIME_TO_3PRIME_BIAS=m$MEDIAN_5PRIME_TO_3PRIME_BIAS,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "    readPicard.duplicate_metrics <- function(source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.Aligned.sortedByCoord.md.metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".Aligned.sortedByCoord.md.metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".Aligned.sortedByCoord.md.metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PERCENT_DUPLICATION=m$PERCENT_DUPLICATION,\n",
    "                                   ESTIMATED_LIBRARY_SIZE=m$ESTIMATED_LIBRARY_SIZE,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    readPicard.wgs_metrics <- function (source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.wgs_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".wgs_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".wgs_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   MEDIAN_COVERAGE=m$MEDIAN_COVERAGE,\n",
    "                                   MAD_COVERAGE=m$MAD_COVERAGE,\n",
    "                                   PCT_EXC_DUPE=m$PCT_EXC_DUPE,\n",
    "                                   PCT_EXC_TOTAL=m$PCT_EXC_TOTAL,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "\n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "    readPicard.insert_size_metrics <- function (source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.insert_size_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".insert_size_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".insert_size_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   MEDIAN_INSERT_SIZE=m$MEDIAN_INSERT_SIZE,\n",
    "                                   MODE_INSERT_SIZE=m$MODE_INSERT_SIZE,\n",
    "                                   MEDIAN_ABSOLUTE_DEVIATION=m$MEDIAN_ABSOLUTE_DEVIATION,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    readPicard.gc_bias.summary_metrics <- function (source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.gc_bias.summary_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".gc_bias.summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".gc_bias.summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   AT_DROPOUT=m$AT_DROPOUT,\n",
    "                                   GC_DROPOUT=m$GC_DROPOUT,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "\n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    readPicard <- function(source) {\n",
    "      metrics.aln <- readPicard.alignment_summary_metrics(source)\n",
    "      metrics.rna <- readPicard.rna_metrics(source)\n",
    "      metrics.dup <- readPicard.duplicate_metrics(source)\n",
    "\n",
    "      stopifnot(all(row.names(metrics.aln) %in% row.names(metrics.rna)) &\n",
    "                all(row.names(metrics.rna) %in% row.names(metrics.dup)) &\n",
    "                all(row.names(metrics.dup) %in% row.names(metrics.aln)))\n",
    "\n",
    "      metrics.aln$File <- NULL\n",
    "      metrics.rna$File <- NULL\n",
    "      metrics.dup$File <- NULL\n",
    "      metrics.rna$Sample <- NULL\n",
    "      metrics.dup$Sample <- NULL\n",
    "\n",
    "      metrics <- cbind(metrics.aln, metrics.rna[row.names(metrics.aln), ])\n",
    "      metrics <- cbind(metrics, metrics.dup[row.names(metrics.aln), ])\n",
    "\n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    ## Execution  \n",
    "    sourcePicard = ${_input[-1]:dr}\n",
    "\n",
    "    Picard_qualityMetrics <- readPicard(sourcePicard)\n",
    "    write.table(Picard_qualityMetrics, file=\"${_output}\",col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.23.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
